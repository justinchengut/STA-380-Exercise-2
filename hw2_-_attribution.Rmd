---
title: "hw2 - attribution"
output: html_document
---

# Read in libaries
First, we simply read in all of the libaries we will use.

```{r}
library(tm)
library(randomForest)
library(e1071)
library(rpart)
library(ggplot2)
library(caret)
library(plyr)

```

# Reader function
This function makes reading plain text simpler by creating a resuable method to handle this for us.
```{r}
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), id=fname, language='en') }

```

# Train on Corpus Data
Next, we train using the Corpus Data in C50train.  We have to obtain data from each of the authors, and all of the files that each author has, and then convert all of this data into a useable format.
```{r}

author_dirs = Sys.glob('data/ReutersC50/C50train/*')
file_list = NULL
train_labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=23)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  train_labels = append(train_labels, rep(author_name, length(files_to_add)))
}

# Named conversion & cleanup
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

#Initialize Training Corpus
train_corpus = Corpus(VectorSource(all_docs))
names(train_corpus) = file_list

#Tokenization of training Corpus
train_corpus = tm_map(train_corpus, content_transformer(tolower)) 
train_corpus = tm_map(train_corpus, content_transformer(removeNumbers)) 
train_corpus = tm_map(train_corpus, content_transformer(removePunctuation)) 
train_corpus = tm_map(train_corpus, content_transformer(stripWhitespace)) 
train_corpus = tm_map(train_corpus, content_transformer(removeWords), stopwords("SMART"))

#Create training DTM & dense matrix
DTM_train = DocumentTermMatrix(train_corpus)
DTM_train = removeSparseTerms(DTM_train, 0.975)

```


# TESTING CORPUS
Next, we use our training model and test based on the test data to help create a better model.
```{r}
author_dirs = Sys.glob('data/ReutersC50/C50test/*')
file_list = NULL
test_labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=22)
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  test_labels = append(test_labels, rep(author_name, length(files_to_add)))
}

# Named conversion & cleanup
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

#Initialize Testing Corpus
test_corpus = Corpus(VectorSource(all_docs))
names(test_corpus) = file_list

#Tokenization of Testing Corpus
test_corpus = tm_map(test_corpus, content_transformer(tolower)) 
test_corpus = tm_map(test_corpus, content_transformer(removeNumbers)) 
test_corpus = tm_map(test_corpus, content_transformer(removePunctuation)) 
test_corpus = tm_map(test_corpus, content_transformer(stripWhitespace)) 
test_corpus = tm_map(test_corpus, content_transformer(removeWords), stopwords("SMART"))


#### Dictionary Creation ####
# We need a dictionary of terms from the training corpus
# in order to extract terms from the test corpus
reuters_dict = NULL
reuters_dict = dimnames(DTM_train)[[2]]

#Create testing DTM & matrix using dictionary words only
DTM_test = DocumentTermMatrix(test_corpus, list(dictionary=reuters_dict))
DTM_test = removeSparseTerms(DTM_test, 0.975)
```

# Create data frames for train and test
Now, we inspect all of the data as a data frame, which shows us all of the terms for each text file for each author.  
```{r}
DTM_train_df = as.data.frame(inspect(DTM_train))
DTM_test_df = as.data.frame(inspect(DTM_test))

```

As we can see, there are a lot of 0's in the data, as most text files will not have even a single occurance of a word in the text file due to the sheer number of words in the dictionary.


# Naive Bayes Model

First, we preform a Naive Bayes Model on the data, and then plot the results.
```{r}

model_NB = naiveBayes(x=DTM_train_df, y=as.factor(train_labels), laplace=1)

pred_NB = predict(model_NB, DTM_test_df)


table_NB = as.data.frame(table(pred_NB,test_labels))


plot = ggplot(table_NB)
plot + geom_tile(aes(x=test_labels, y=pred_NB, fill=Freq)) + 
    scale_x_discrete(name="Actual Class") + 
    scale_y_discrete(name="Predicted Class") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

This graph shows us the frequency that the predicted class matched the actual class.  While this model does a decent job predicting authors, we can see a large number of cases where the model incorrectly predicted the author as seen by the frequencies, suggesting that while this model is good, there is likely a better model.


# Random Forest Model
Next, we create a Random Forest Model using the same data to see how accurate this model is.
First we need to add empty columns in the test data set for words that appear in the training data, but not in the test data. RandomForest requires the same variables in training and test sets.

```{r}

#First we need to add empty columns in the test data set for words that appear in
#the training data, but not in the test data. RandomForest requires the same variables
#in training and test sets

DTM_test = as.matrix(DTM_test)
DTM_train = as.matrix(DTM_train)

xx <- data.frame(DTM_test[,intersect(colnames(DTM_test), colnames(DTM_train))])
yy <- read.table(textConnection(""), col.names = colnames(DTM_train), colClasses = "integer")
DTM_test_clean = rbind.fill(xx, yy)

DTM_test_df = as.data.frame(DTM_test_clean)


model_RF = randomForest(x=DTM_train_df, y=as.factor(train_labels), mtry=3, ntree=250)
pred_RF = predict(model_RF, data=DTM_test_clean)

table_RF = as.data.frame(table(pred_RF,test_labels))

plot = ggplot(table_RF)
plot + geom_tile(aes(x=test_labels, y=pred_RF, fill=Freq)) + 
  scale_x_discrete(name="Actual Class") + 
  scale_y_discrete(name="Predicted Class") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

The graph that this model produces shows a much better prediction of the authors.  In most cases, the Random Forest model predicted a large majority of the authors, suggesting that Random Forest is likely the better model to use in this case.